{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from unsloth import FastVisionModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "if os.environ.get(\"HUGGINGFACE_TOKEN\") is None:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's try to download the dataset from HuggingFace\n",
    "# We don't need to use a token, because it's a public dataset.\n",
    "dataset_name = \"UCSC-Admire/idiom-dataset-100-2024-11-11_14-37-58\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversation(sample: dict):\n",
    "    # 1. Create list of (original_position, image) pairs\n",
    "    original_images = [\n",
    "        (i, sample[f\"image_{i}\"])\n",
    "        for i in range(1, 6)\n",
    "    ]\n",
    "    \n",
    "    # 2. Create a shuffled version of this list\n",
    "    shuffled_images = original_images.copy()\n",
    "    random.shuffle(shuffled_images)\n",
    "    \n",
    "    # 3. Assign letters A-E to the shuffled images\n",
    "    letters = list(\"ABCDE\")\n",
    "    images_with_letters = [\n",
    "        (letter, image)\n",
    "        for letter, (_, image) in zip(letters, shuffled_images)\n",
    "    ]\n",
    "    \n",
    "    # 4. Create mapping of original_position to assigned letter\n",
    "    original_to_letter = {\n",
    "        orig_pos: letter\n",
    "        for (orig_pos, _), (letter, _) in zip(shuffled_images, images_with_letters)\n",
    "    }\n",
    "    \n",
    "    # Now the correct order is the letters assigned to positions 1,2,3,4,5\n",
    "    correct_order = [original_to_letter[i] for i in range(1, 6)]\n",
    "\n",
    "    # print(f\"Original order: {original_images}\\n\")\n",
    "    # print(f\"Shuffled order: {shuffled_images}\\n\")\n",
    "    # print(f\"Shuffled order with letters: {images_with_letters}\\n\")\n",
    "    # print(f\"Original to letter: {original_to_letter}\\n\")\n",
    "    # print(f\"Correct order: {correct_order}\\n\")\n",
    "    \n",
    "    instruction = f\"\"\"You are given a compound, its use in a sentence (which determines whether a compound should be interpreted literally or idiomatically), and five images.\n",
    "    The images have been given aliases of {', '.join(letters)}, respectively.\n",
    "    Rank the images from most to least relevant, based on how well they represent the compound (in either a literal or idiomatic sense, based on how it's used in the sentence).\n",
    "    Return the ranking of the images as a comma-separated list of the aliases, from most to least relevant.\n",
    "    \n",
    "    As an example, if your predicted ranking from most to least relevant is B, C, A, E, D, then you should respond with \"B, C, A, E, D\".\"\"\"\n",
    "\n",
    "    correct_response = f\"{', '.join(correct_order)}\"\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                *[{\"type\": \"image\", \"image\": img} for _, img in shuffled_images]\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": correct_response}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"messages\": conversation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Qwen2_Vl vision patching. Transformers: 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu118. CUDA: 8.9. CUDA Toolkit: 11.8. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/code/ucsc/243/admire/admire-finetuning-sam/venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    }
   ],
   "source": [
    "# Load our model\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    dtype=torch.bfloat16,  # Load in bfloat16\n",
    ")\n",
    "\n",
    "# It looks like \"We also support finetuning ONLY the vision part of the model, or ONLY the langauge part\"\n",
    "# Or you can select both! You can also select to finetune the attention or the MLP layers\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,  # Leave the vision layers frozen\n",
    "    finetune_language_layers=True,  # Finetune the language layers\n",
    "    finetune_attention_modules=True,  # Finetune the attention modules\n",
    "    finetune_mlp_modules=True,  # Finetune the MLP modules\n",
    "\n",
    "    r=16,  # Rank of the LoRA matrices; Larger, the higher the acc, but might overfit\n",
    "    lora_alpha=16,  # Recommneded alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,  # They support rank stabilized LoRA\n",
    "    loftq_config = None,  # They also support LoftQ\n",
    "    # target_modules = \"all-linear\"  # Optional now; can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 original size: (1024, 1024)\n",
      "Image 1 resized to: (224, 224)\n",
      "----------------------------------------\n",
      "Image 2 original size: (1024, 1024)\n",
      "Image 2 resized to: (224, 224)\n",
      "----------------------------------------\n",
      "Image 3 original size: (1024, 1024)\n",
      "Image 3 resized to: (224, 224)\n",
      "----------------------------------------\n",
      "Image 4 original size: (1024, 1024)\n",
      "Image 4 resized to: (224, 224)\n",
      "----------------------------------------\n",
      "Image 5 original size: (1024, 1024)\n",
      "Image 5 resized to: (224, 224)\n",
      "----------------------------------------\n",
      "B, C, A, E, D<|im_end|>\n",
      "{'role': 'assistant', 'content': [{'type': 'text', 'text': 'C, D, E, B, A'}]}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Let's try for one sample!\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images(images, max_size=224):\n",
    "    \"\"\"Resize a list of PIL images while maintaining aspect ratio\"\"\"\n",
    "    resized_images = []\n",
    "    for i, img in enumerate(images, 1):\n",
    "        # Print original size\n",
    "        print(f\"Image {i} original size: {img.size}\")\n",
    "        \n",
    "        # Calculate new size maintaining aspect ratio\n",
    "        ratio = max_size/max(img.size)\n",
    "        new_size = tuple([int(x*ratio) for x in img.size])\n",
    "        \n",
    "        # Print new size\n",
    "        print(f\"Image {i} resized to: {new_size}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        resized_images.append(img.resize(new_size, Image.Resampling.LANCZOS))\n",
    "    return resized_images\n",
    "\n",
    "\n",
    "record = dataset[0]\n",
    "images = [record[f\"image_{i}\"] for i in range(1, 6)]\n",
    "images = resize_images(images)\n",
    "messages = convert_to_conversation(record)[\"messages\"][:1]\n",
    "answer = convert_to_conversation(record)[\"messages\"][1]\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False  # Don't tokenize yet to avoid double tokenization\n",
    ").replace(\"<|im_end|>\", \"\")  # Remove the im_end token\n",
    "\n",
    "inputs = tokenizer(\n",
    "    images,\n",
    "    input_text,\n",
    "    add_special_tokens=True,  # Changed to True to properly handle special tokens\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    # pad_token_id=tokenizer.pad_token_id,\n",
    "    # eos_token_id=tokenizer.eos_token_id  # Explicitly set EOS token\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
