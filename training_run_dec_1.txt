This is the result of the training run that created UCSC-Admire/Admire-Finetune-202-12-01_22-21-53
That model was trained using

SFTConfig(
        per_device_train_batch_size = 1,  # SAmples processed per GPU forward pass # Even thuogh bs=1, we accumumulate gradients over 8 steps
        gradient_accumulation_steps = 8,  # Accumulate gradients over 8 steps before updating
        warmup_steps = 5,  # Number of steps over which we gradually increase learning rate (bs*gradient_acc*warmupsteps)
        # max_steps = 30,  # Total training steps (gradient updates); alternative to the 
        num_train_epochs = 1, # Set this instead of max_steps for full training runs
        learning_rate = 2e-4,  # Initial learning rate
        fp16 = not is_bf16_supported(),  # Uses 16-bit floating point if bfloat16 isn't available
        bf16 = is_bf16_supported(),  # Uses bfloat16 if supported (better numerical stability)
        logging_steps = 1,
        optim = "adamw_8bit",  # 8-bit AdamW optimizer for memory efficiency
        weight_decay = 0.01,  # L2 regularization to fight overfitting
        lr_scheduler_type = "linear",  # Learning rate decreases linearly to zero
        seed = 3407,
        output_dir = "outputs",  # Directory to save model checkpoints
        report_to = "none",     # For Weights and Biases
        evaluation_strategy = "steps",
        eval_steps = 10,
        per_device_eval_batch_size=1,
        # You MUST put the below items for vision finetuning:
        remove_unused_columns = False,
        dataset_text_field = "",
        dataset_kwargs = {"skip_prepare_dataset": True},
        dataset_num_proc = 4,
        max_seq_length = 2048,
    ),

and it seems to me from looking at the loss below that... we might be overfitting a little bit?
The eval loss hits a minimum and then seems to rise for two evaluations in a row... which is worrisome.
It would really help to graph out the training and evaluation loss.

(venv) sam@pop-os:~/code/ucsc/243/admire/admire-finetuning-sam$ /home/sam/code/ucsc/243/admire/admire-finetuning-sam/venv/bin/python /home/sam/code/ucsc/243/admire/admire-finetuning-sam/train.py
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Loading model...
<string>:204: SyntaxWarning: invalid escape sequence '\ '
<string>:205: SyntaxWarning: invalid escape sequence '\_'
<string>:206: SyntaxWarning: invalid escape sequence '\ '
<string>:204: SyntaxWarning: invalid escape sequence '\ '
<string>:205: SyntaxWarning: invalid escape sequence '\_'
<string>:206: SyntaxWarning: invalid escape sequence '\ '
==((====))==  Unsloth 2024.11.10: Fast Qwen2_Vl vision patching. Transformers: 4.46.2.
   \\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu118. CUDA: 8.9. CUDA Toolkit: 11.8. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading PEFT model...
Loading dataset...
Converting dataset to conversation format...
Beginning training...
/home/sam/code/ucsc/243/admire/admire-finetuning-sam/venv/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 540 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 1 | Gradient Accumulation steps = 8
\        /    Total batch size = 8 | Total steps = 67
 "-____-"     Number of trainable parameters = 18,464,768
ðŸ¦¥ Unsloth needs about 1-3 minutes to load everything - please wait!
{'loss': 2.1974, 'grad_norm': 1.2935400009155273, 'learning_rate': 4e-05, 'epoch': 0.01}                       
{'loss': 2.2008, 'grad_norm': 1.297235369682312, 'learning_rate': 8e-05, 'epoch': 0.03}                        
{'loss': 2.1894, 'grad_norm': 1.274440884590149, 'learning_rate': 0.00012, 'epoch': 0.04}                      
{'loss': 2.1247, 'grad_norm': 1.1771882772445679, 'learning_rate': 0.00016, 'epoch': 0.06}                     
{'loss': 1.9726, 'grad_norm': 1.0483499765396118, 'learning_rate': 0.0002, 'epoch': 0.07}                      
{'eval_loss': 1.7482972145080566, 'eval_runtime': 52.4678, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 1.144, 'epoch': 0.07}                                                                                   
{'loss': 1.7432, 'grad_norm': 1.0441805124282837, 'learning_rate': 0.0001967741935483871, 'epoch': 0.09}       
{'loss': 1.4813, 'grad_norm': 1.074625849723816, 'learning_rate': 0.00019354838709677422, 'epoch': 0.1}        
{'loss': 1.2408, 'grad_norm': 1.1303483247756958, 'learning_rate': 0.0001903225806451613, 'epoch': 0.12}       
{'loss': 1.0092, 'grad_norm': 1.1301188468933105, 'learning_rate': 0.0001870967741935484, 'epoch': 0.13}       
{'loss': 0.7963, 'grad_norm': 1.494836449623108, 'learning_rate': 0.00018387096774193548, 'epoch': 0.15}       
{'eval_loss': 0.617742657661438, 'eval_runtime': 51.9268, 'eval_samples_per_second': 1.155, 'eval_steps_per_second': 1.155, 'epoch': 0.15}                                                                                    
{'loss': 0.6142, 'grad_norm': 1.7095742225646973, 'learning_rate': 0.00018064516129032257, 'epoch': 0.16}      
{'loss': 0.4425, 'grad_norm': 1.3477681875228882, 'learning_rate': 0.0001774193548387097, 'epoch': 0.18}       
{'loss': 0.288, 'grad_norm': 1.155230164527893, 'learning_rate': 0.00017419354838709678, 'epoch': 0.19}        
{'loss': 0.1667, 'grad_norm': 1.1605669260025024, 'learning_rate': 0.0001709677419354839, 'epoch': 0.21}       
{'loss': 0.1127, 'grad_norm': 0.31226369738578796, 'learning_rate': 0.00016774193548387098, 'epoch': 0.22}     
{'eval_loss': 0.0998021811246872, 'eval_runtime': 51.5238, 'eval_samples_per_second': 1.165, 'eval_steps_per_second': 1.165, 'epoch': 0.22}                                                                                   
{'loss': 0.1011, 'grad_norm': 0.29658475518226624, 'learning_rate': 0.00016451612903225807, 'epoch': 0.24}     
{'loss': 0.0964, 'grad_norm': 0.1754477471113205, 'learning_rate': 0.00016129032258064516, 'epoch': 0.25}      
{'loss': 0.0933, 'grad_norm': 0.1579480767250061, 'learning_rate': 0.00015806451612903225, 'epoch': 0.27}      
{'loss': 0.0913, 'grad_norm': 0.1804446578025818, 'learning_rate': 0.00015483870967741937, 'epoch': 0.28}      
{'loss': 0.0885, 'grad_norm': 0.20383374392986298, 'learning_rate': 0.00015161290322580646, 'epoch': 0.3}      
{'eval_loss': 0.08808466792106628, 'eval_runtime': 51.4118, 'eval_samples_per_second': 1.167, 'eval_steps_per_second': 1.167, 'epoch': 0.3}                                                                                   
{'loss': 0.0891, 'grad_norm': 0.21759477257728577, 'learning_rate': 0.00014838709677419355, 'epoch': 0.31}     
{'loss': 0.0853, 'grad_norm': 0.24543273448944092, 'learning_rate': 0.00014516129032258066, 'epoch': 0.33}     
{'loss': 0.0829, 'grad_norm': 0.2826456129550934, 'learning_rate': 0.00014193548387096775, 'epoch': 0.34}      
{'loss': 0.0818, 'grad_norm': 0.31181639432907104, 'learning_rate': 0.00013870967741935487, 'epoch': 0.36}     
{'loss': 0.0773, 'grad_norm': 0.3945501446723938, 'learning_rate': 0.00013548387096774193, 'epoch': 0.37}      
{'eval_loss': 0.07250455021858215, 'eval_runtime': 51.3465, 'eval_samples_per_second': 1.169, 'eval_steps_per_second': 1.169, 'epoch': 0.37}                                                                                  
{'loss': 0.0712, 'grad_norm': 0.4971521496772766, 'learning_rate': 0.00013225806451612905, 'epoch': 0.39}      
{'loss': 0.0596, 'grad_norm': 0.7333290576934814, 'learning_rate': 0.00012903225806451613, 'epoch': 0.4}       
{'loss': 0.0467, 'grad_norm': 1.0164551734924316, 'learning_rate': 0.00012580645161290322, 'epoch': 0.41}      
{'loss': 0.0298, 'grad_norm': 0.15562279522418976, 'learning_rate': 0.00012258064516129034, 'epoch': 0.43}     
{'loss': 0.0311, 'grad_norm': 0.07899700850248337, 'learning_rate': 0.00011935483870967743, 'epoch': 0.44}     
{'eval_loss': 0.029804622754454613, 'eval_runtime': 51.0693, 'eval_samples_per_second': 1.175, 'eval_steps_per_second': 1.175, 'epoch': 0.44}                                                                                 
{'loss': 0.0287, 'grad_norm': 0.06603677570819855, 'learning_rate': 0.00011612903225806453, 'epoch': 0.46}     
{'loss': 0.0291, 'grad_norm': 0.08468225598335266, 'learning_rate': 0.00011290322580645163, 'epoch': 0.47}     
{'loss': 0.0289, 'grad_norm': 0.06124073266983032, 'learning_rate': 0.00010967741935483871, 'epoch': 0.49}     
{'loss': 0.0297, 'grad_norm': 0.12255310267210007, 'learning_rate': 0.0001064516129032258, 'epoch': 0.5}       
{'loss': 0.0287, 'grad_norm': 0.047760188579559326, 'learning_rate': 0.0001032258064516129, 'epoch': 0.52}     
{'eval_loss': 0.0282726027071476, 'eval_runtime': 51.1031, 'eval_samples_per_second': 1.174, 'eval_steps_per_second': 1.174, 'epoch': 0.52}                                                                                   
{'loss': 0.0286, 'grad_norm': 0.06219858303666115, 'learning_rate': 0.0001, 'epoch': 0.53}                     
{'loss': 0.0273, 'grad_norm': 0.05240324139595032, 'learning_rate': 9.677419354838711e-05, 'epoch': 0.55}      
{'loss': 0.0304, 'grad_norm': 0.06781325489282608, 'learning_rate': 9.35483870967742e-05, 'epoch': 0.56}       
{'loss': 0.0285, 'grad_norm': 0.06488651037216187, 'learning_rate': 9.032258064516129e-05, 'epoch': 0.58}      
{'loss': 0.0297, 'grad_norm': 0.06912577152252197, 'learning_rate': 8.709677419354839e-05, 'epoch': 0.59}      
{'eval_loss': 0.027590034529566765, 'eval_runtime': 51.8339, 'eval_samples_per_second': 1.158, 'eval_steps_per_second': 1.158, 'epoch': 0.59}                                                                                 
{'loss': 0.0279, 'grad_norm': 0.060365818440914154, 'learning_rate': 8.387096774193549e-05, 'epoch': 0.61}     
{'loss': 0.0279, 'grad_norm': 0.04936236888170242, 'learning_rate': 8.064516129032258e-05, 'epoch': 0.62}      
{'loss': 0.029, 'grad_norm': 0.0632537379860878, 'learning_rate': 7.741935483870968e-05, 'epoch': 0.64}        
{'loss': 0.0293, 'grad_norm': 0.0679188147187233, 'learning_rate': 7.419354838709677e-05, 'epoch': 0.65}       
{'loss': 0.0283, 'grad_norm': 0.07021041214466095, 'learning_rate': 7.096774193548388e-05, 'epoch': 0.67}      
{'eval_loss': 0.027466753497719765, 'eval_runtime': 51.2202, 'eval_samples_per_second': 1.171, 'eval_steps_per_second': 1.171, 'epoch': 0.67}                                                                                 
{'loss': 0.0272, 'grad_norm': 0.05224897712469101, 'learning_rate': 6.774193548387096e-05, 'epoch': 0.68}      
{'loss': 0.0285, 'grad_norm': 0.06398706138134003, 'learning_rate': 6.451612903225807e-05, 'epoch': 0.7}       
{'loss': 0.028, 'grad_norm': 0.05698835477232933, 'learning_rate': 6.129032258064517e-05, 'epoch': 0.71}       
{'loss': 0.0282, 'grad_norm': 0.050691261887550354, 'learning_rate': 5.8064516129032266e-05, 'epoch': 0.73}    
{'loss': 0.03, 'grad_norm': 0.06701323390007019, 'learning_rate': 5.4838709677419355e-05, 'epoch': 0.74}       
{'eval_loss': 0.028245266526937485, 'eval_runtime': 51.5041, 'eval_samples_per_second': 1.165, 'eval_steps_per_second': 1.165, 'epoch': 0.74}                                                                                 
{'loss': 0.0298, 'grad_norm': 0.09106099605560303, 'learning_rate': 5.161290322580645e-05, 'epoch': 0.76}      
{'loss': 0.0282, 'grad_norm': 0.046501800417900085, 'learning_rate': 4.8387096774193554e-05, 'epoch': 0.77}    
{'loss': 0.0277, 'grad_norm': 0.04401141777634621, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.79}      
{'loss': 0.0299, 'grad_norm': 0.05446844920516014, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.8}      
{'loss': 0.0278, 'grad_norm': 0.060192205011844635, 'learning_rate': 3.870967741935484e-05, 'epoch': 0.81}     
{'eval_loss': 0.028233971446752548, 'eval_runtime': 51.3192, 'eval_samples_per_second': 1.169, 'eval_steps_per_second': 1.169, 'epoch': 0.81}                                                                                 
{'loss': 0.0272, 'grad_norm': 0.040944766253232956, 'learning_rate': 3.548387096774194e-05, 'epoch': 0.83}     
{'loss': 0.0274, 'grad_norm': 0.029892602935433388, 'learning_rate': 3.2258064516129034e-05, 'epoch': 0.84}    
{'loss': 0.0292, 'grad_norm': 0.05933721736073494, 'learning_rate': 2.9032258064516133e-05, 'epoch': 0.86}     
{'loss': 0.0273, 'grad_norm': 0.04924444854259491, 'learning_rate': 2.5806451612903226e-05, 'epoch': 0.87}     
{'loss': 0.028, 'grad_norm': 0.05184661224484444, 'learning_rate': 2.258064516129032e-05, 'epoch': 0.89}       
{'eval_loss': 0.028421325609087944, 'eval_runtime': 51.0054, 'eval_samples_per_second': 1.176, 'eval_steps_per_second': 1.176, 'epoch': 0.89}                                                                                 
{'loss': 0.029, 'grad_norm': 0.07048220187425613, 'learning_rate': 1.935483870967742e-05, 'epoch': 0.9}        
{'loss': 0.0279, 'grad_norm': 0.05428435280919075, 'learning_rate': 1.6129032258064517e-05, 'epoch': 0.92}     
{'loss': 0.0281, 'grad_norm': 0.04479655623435974, 'learning_rate': 1.2903225806451613e-05, 'epoch': 0.93}     
{'loss': 0.0296, 'grad_norm': 0.06067022681236267, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.95}       
{'loss': 0.0274, 'grad_norm': 0.04747074469923973, 'learning_rate': 6.451612903225806e-06, 'epoch': 0.96}      
{'eval_loss': 0.02849758416414261, 'eval_runtime': 50.9825, 'eval_samples_per_second': 1.177, 'eval_steps_per_second': 1.177, 'epoch': 0.96}                                                                                  
{'loss': 0.0289, 'grad_norm': 0.043320585042238235, 'learning_rate': 3.225806451612903e-06, 'epoch': 0.98}     
{'loss': 0.0281, 'grad_norm': 0.04174443706870079, 'learning_rate': 0.0, 'epoch': 0.99}                        
{'train_runtime': 1204.3538, 'train_samples_per_second': 0.448, 'train_steps_per_second': 0.056, 'train_loss': 0.30985578958556725, 'epoch': 0.99}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [20:04<00:00, 17.98s/it]
Unsloth: Merging QLoRA weights directly to the 16bit version of unsloth/Qwen2-VL-2B-Instruct.
Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.25s/it]
Unsloth: Merging QLoRA weights directly to the 16bit version of unsloth/Qwen2-VL-2B-Instruct.
Unsloth: Merging weights into 16bit:   0%|                                               | 0/1 [00:00<?, ?it/s]
  0%|                                                                                    | 0/1 [00:00<?, ?it/s]
model.safetensors: 4.43GB [33:12, 2.22MB/s]                    
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [33:12<00:00, 1992.88s/it]
Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [33:21<00:00, 2001.26s/it]